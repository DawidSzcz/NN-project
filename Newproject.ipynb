{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<CudaNdarrayType(float32, vector)>), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.257601 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled)\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        #print 'u', shape\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "\n",
    "                         \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset=slice(None,40000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(\n",
    "    cifar10_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar10_train.num_examples, 100))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset=slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(\n",
    "    cifar10_validation, iteration_scheme=SequentialScheme(cifar10_validation.num_examples, 250))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(\n",
    "    cifar10_test, iteration_scheme=SequentialScheme(cifar10_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 1, 28, 28) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (250, 1, 28, 28) containing float32\n",
      " - an array of size (250, 1) containing uint8\n",
      "CIFAR: \n",
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (250, 3, 32, 32) containing float32\n",
      " - an array of size (250, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"CIFAR: \"  \n",
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-6604dab46423>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-6604dab46423>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    iris_train_f = iris[:2*pop_num/3,1:]\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#print iris\n",
    "\n",
    "feats = 4\n",
    "alpha = 0.01\n",
    "pop_num = 150\n",
    "rng = np.random\n",
    "iris_f = iris['data'][:pop_num,:feats]\n",
    "iris_t = iris['target'][:pop_num]\n",
    "iris = hstack(([[x] for x in iris_t], iris_f))\n",
    "\n",
    "rng.shuffle(iris\n",
    "\n",
    "#print iris\n",
    "\n",
    "iris_train_f = iris[:2*pop_num/3,1:]\n",
    "iris_train_t = np.array(iris[:2*pop_num/3, 0], dtype='uint8')\n",
    "iris_test_f = iris[2*pop_num/3:,1:]\n",
    "iris_test_t = np.array(iris[2*pop_num/3:, 0], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.printing as TP\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor.signal.downsample as down\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, lamb = 0.1,rng=None, name=\"\"):\n",
    "        self.name = name\n",
    "        self.lamb = lamb\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "    def update(self, foo, alpha):\n",
    "        return []\n",
    "    def cost(self):\n",
    "        return 0;\n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = inputDim\n",
    "    def getOutputDim(self):\n",
    "        return self.num_out\n",
    "    def setMoments(self, moments):\n",
    "        self.moments = moments\n",
    "    def setLambda(self, lamb):\n",
    "        self.lamb = lamb\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_out, initW = 10., gamma  = 0.1, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(initW / (num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.b = theano.shared(bias_init.generate(self.rng, (num_out)), name=self.name +\" bias\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    @property\n",
    "    def parametersValues(self):\n",
    "        return [self.W.get_value(), self.b.get_value()]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def build(self, X):\n",
    "        #print self.name+ \" \",X.shape \n",
    "        return X.dot(self.W) + self.b\n",
    "    def cost(self):\n",
    "        return  (self.W ** 2).sum() * self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw, gb = T.grad(foo, self.parameters)\n",
    "        moments = self.moments\n",
    "        self.setMoments((gw, gb))\n",
    "        return  [(self.W, self.W - (alpha * gw + self.lamb * moments[0])), \n",
    "                 (self.b, self.b - (alpha * gb+ self.lamb * moments[1]))]\n",
    "    def setInputDim(self, inputDim):\n",
    "        shape = (inputDim, self.num_out)\n",
    "        print \"AffineLayer: \", shape\n",
    "        self.W = theano.shared(self.weight_init.generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.setMoments(zeros(shape, dtype='float32'))\n",
    "    \n",
    "class LogRegLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(LogRegLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.nnet.sigmoid(X)\n",
    "\n",
    "\n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(TanhLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        print \"tanh layer\", X\n",
    "        return T.tanh(X)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(ReLULayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, X)\n",
    "\n",
    "class Conv(Layer):\n",
    "    def __init__(self, f_out, f_size, initW = 10., gamma = 0.1, n = \"\", weight_init = None, **kwargs):\n",
    "        super(Conv, self).__init__(name = n, **kwargs)\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(initW / (f_out+ f_size + f_size))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.f_out = f_out\n",
    "        self.f_size = f_size\n",
    "    \n",
    "    \n",
    "    def setInputDim(self, inputDim):\n",
    "        F_size = (self.f_out, ) + (inputDim[0], self.f_size, self.f_size)                                   \n",
    "        self.num_out = (self.f_out, inputDim[1] - self.f_size + 1, inputDim[2] - self.f_size + 1)\n",
    "        print 'Conv filter', F_size\n",
    "        self.F = theano.shared(self.weight_init.generate(self.rng, F_size),name=self.name +\" filter\")\n",
    "        \n",
    "    def update(self, foo, alpha):\n",
    "        gf = T.grad(foo, self.F)\n",
    "        return  [(self.F, self.F - alpha * gf)]    \n",
    "    \n",
    "    #def cost(self):\n",
    "    #    return  (self.F ** 2).sum() * self.gamma\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, T.nnet.conv2d(X, self.F))\n",
    "        \n",
    "        \n",
    "        \n",
    "class Flatten(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(Flatten, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.flatten(X, 2)\n",
    "    def setInputDim(self, inputDim):\n",
    "        out_dim = 1\n",
    "        for i in inputDim:\n",
    "            out_dim = out_dim * i\n",
    "        self.num_out = out_dim\n",
    "    \n",
    "\n",
    "class BNLayer(Layer):\n",
    "    def __init__(self,num_out, n = \"BNLayer\", gamma = 0.1, alpha=1.0,**kwargs):\n",
    "        super(BNLayer, self).__init__(name = n, **kwargs)\n",
    "        self.num_out, self.alpha = num_out, alpha\n",
    "        self.gamma= theano.shared(gamma)\n",
    "    def build(self, X):\n",
    "        self.Gamma = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Gamma \" + self.name))\n",
    "        print 'Gamma shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Beta  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Beta \" + self.name))\n",
    "        print 'Beta shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Gamma.tag.initializer = Constant(1.0)\n",
    "        self.Beta.tag.initializer = Constant(0.0)\n",
    "    \n",
    "        #self.stored_means = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Means\" + self.name))\n",
    "        #self.stored_stds  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Stds\" + self.name))\n",
    "        #self.stored_means.tag.initializer = Constant(0.0)\n",
    "        #self.stored_stds.tag.initializer = Constant(1.0)\n",
    "    \n",
    "        self.means = self.alpha * theano.tensor.mean(X, 0, keepdims=True)\n",
    "        self.stds = self.alpha * theano.tensor.std(X, 0, keepdims=True)\n",
    "        self.means.tag.initializer = Constant(0.0)\n",
    "        self.stds.tag.initializer = Constant(1.0)\n",
    "        #self.means = self.alpha *self.means + (1.0 - self.alpha) * self.stored_means.dimshuffle(0,'x')\n",
    "        #self.stds = self.alpha * self.stds + (1.0 - self.alpha) * self.stored_stds.dimshuffle(0,'x')\n",
    "        \n",
    "        normalized = theano.tensor.nnet.bn.batch_normalization(\n",
    "            X,\n",
    "            self.Gamma,\n",
    "            self.Beta,\n",
    "            self.means,\n",
    "            self.stds,\n",
    "            'high_mem'\n",
    "        )\n",
    "        return normalized\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.Gamma, self.Beta]\n",
    "    @property\n",
    "    def check(self):\n",
    "        return [self.gg, self.gb, self.Gamma, self.Beta,self.means, self.stds ]\n",
    "    #def cost(self):\n",
    "    #    return  ((self.Gamma ** 2).sum() + (self.Gamma ** 2).sum())* self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        self.gg, self.gb = T.grad(foo, self.parameters)\n",
    "        return  [(self.Gamma, self.Gamma- alpha *self.gg),\n",
    "            (self.Beta, self.Beta - alpha * self.gb)] \n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.nnet.softmax(X)\n",
    "\n",
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, p_size, n = \"MP\", **kwargs):\n",
    "        super(MaxPoolLayer, self).__init__(name = n, **kwargs)\n",
    "        self.p_size = p_size\n",
    "    def build(self, input):\n",
    "        return down.max_pool_2d(input, (self.p_size,self.p_size), ignore_border=True)\n",
    "    def getOutputDim(self):\n",
    "        shape = (self.num_out[0], ) + (self.num_out[1]/self.p_size, self.num_out[2]/self.p_size) \n",
    "        print \"maxPool\", shape\n",
    "        return shape\n",
    "    \n",
    "class DropOutLayer(Layer):\n",
    "    def __init__(self, dropOut = 0.1, n = \"MP\", **kwargs):\n",
    "        super(DropOutLayer, self).__init__(name = n, **kwargs)\n",
    "        self.dropOut = dropOut\n",
    "        self.u = Uniform(0.5, 1.)\n",
    "    def build(self, input):\n",
    "        self.D = theano.shared((self.u.generate(self.rng, (self.num_out,))>= self.dropOut) + 0,name=self.name +\" Dropout\") \n",
    "        print self.D.get_value()\n",
    "        return input * self.D\n",
    "    def getOutputDim(self):\n",
    "        shape = self.num_out \n",
    "        print \"maxPool\", shape\n",
    "        return shape\n",
    "    def update(self, foo, alpha):\n",
    "        return  [(self.D,\n",
    "            (self.u.generate(self.rng, (self.num_out,)) >= self.dropOut)+0)]\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None, alpha=0.1, lamb = 0.1):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "        print type(alpha)\n",
    "        self.alpha = theano.shared(float32(alpha), name='alpha')\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def build(self, inputDim):\n",
    "        x = T.tensor4(\"x\")\n",
    "        y = T.vector(\"y\", dtype='int64')\n",
    "        cost = 0\n",
    "        paramUpdates = []\n",
    "        \n",
    "        X = x\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            #print inputDim\n",
    "            layer.setInputDim(inputDim)\n",
    "            layer.setLambda(lamb)\n",
    "            inputDim = layer.getOutputDim()\n",
    "            X = layer.build(X)\n",
    "            #if(i == 3):\n",
    "            #    D = X\n",
    "            cost += layer.cost()\n",
    "        \n",
    "        pred = np.argmax(X, 1)\n",
    "        self.costFoo = T.nnet.categorical_crossentropy(X, y).mean() + cost\n",
    "        \n",
    "        #svgdotprint(self.costFoo)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            paramUpdates += layer.update(self.costFoo, self.alpha)\n",
    "        \n",
    "        paramUpdates += [(self.alpha, self.alpha * 0.99993)]\n",
    "        self.train = theano.function(inputs=[x,y], \n",
    "                                    outputs=[pred, self.costFoo, self.alpha],\n",
    "                                    updates=paramUpdates)\n",
    "        self.predict  = theano.function(inputs=[x], \n",
    "                                    outputs=pred)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def trainFunction(self):\n",
    "        return self.train\n",
    "    \n",
    "    @property\n",
    "    def predictFunction(self):\n",
    "        return self.predict\n",
    "    @property\n",
    "    def costFunction(self):\n",
    "        return self.costFoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_er(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        predictions = net.predictFunction(X)\n",
    "        #print predictions != Y.ravel()\n",
    "        num_errs += (predictions != Y.ravel()).sum()\n",
    "        #print Y.shape[0], num_errs\n",
    "        num_examples += Y.shape[0]\n",
    "    return num_errs/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <type 'float'>\n",
      "Conv filter (20, 3, 5, 5)\n",
      "maxPool (20, 9, 9)\n",
      "Conv filter (40, 20, 5, 5)\n",
      "AffineLayer:  (1000, 500)\n",
      "AffineLayer:  (500, 1000)\n",
      "AffineLayer:  (1000, 10)\n",
      "Start\n",
      "gamma:  0.01\n",
      "alpha:  0.005\n",
      "22.387253952 0.78\n",
      "21.8155486107 0.63\n",
      "21.2939832258 0.55\n",
      "20.8205234814 0.56\n",
      "After epoch:  0 0.5988 time:  11.5790638924\n",
      "20.5381700087 0.57\n",
      "20.1330384636 0.5\n",
      "19.9208042812 0.58\n",
      "19.3910835934 0.53\n",
      "After epoch:  1 0.5526 time:  11.5683820248\n",
      "18.862948103 0.46\n",
      "18.6502162266 0.52\n",
      "18.4663848734 0.58\n",
      "18.1054138851 0.54\n",
      "After epoch:  2 0.5183 time:  11.567499876\n",
      "17.6029868269 0.45\n",
      "17.4562418699 0.59\n",
      "17.0683274746 0.47\n",
      "16.8309161282 0.51\n",
      "After epoch:  3 0.5051 time:  11.5688889027\n",
      "16.5115652418 0.49\n",
      "16.087076683 0.4\n",
      "15.9467263174 0.49\n",
      "15.468911438 0.35\n",
      "After epoch:  4 0.4868 time:  11.5675930977\n",
      "15.2993978691 0.4\n",
      "14.8421323848 0.26\n",
      "14.9537456799 0.46\n",
      "14.7467283726 0.48\n",
      "After epoch:  5 "
     ]
    }
   ],
   "source": [
    "img_size = (32, 32)\n",
    "c1_i = 3\n",
    "c1_o = 20\n",
    "c1_f = 5\n",
    "p1   = 3\n",
    "c2_o = 40\n",
    "c2_f = 5\n",
    "hidden1 = 500\n",
    "hidden2 = 600\n",
    "hidden3 = 900\n",
    "hidden4 = 1000\n",
    "hidden5 = 500\n",
    "outs = 10\n",
    "gamma = 0.01\n",
    "alpha = 0.005\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 300\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c1_f, initC, \"Conv2\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, initW, gamma, \"tA\"),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, initW, gamma, \"tA\"),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a = net.trainFunction(X, Y.ravel()) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'float'>\n",
      "Conv filter (50, 3, 5, 5)\n",
      "maxPool (50, 9, 9)\n",
      "Conv filter (100, 50, 5, 5)\n",
      "AffineLayer:  (2500, 1500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (1500, 1200)\n",
      "AffineLayer:  (1200, 2500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (2500, 2000)\n",
      "Gamma shape: (1, 2000)\n",
      "Beta shape: (1, 2000)\n",
      "AffineLayer:  (2000, 1000)\n",
      "AffineLayer:  (1000, 10)\n",
      "Start\n",
      "gamma:  0.003\n",
      "alpha:  0.005\n",
      "28.8414355469 0.73\n",
      "28.5769722729 0.71\n",
      "28.2626245413 0.7\n",
      "28.055165803 0.68\n",
      "After epoch:  0 0.6709 time:  27.6519670486\n",
      "27.7894599752 0.58\n",
      "27.5488911686 0.62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f2659f725bae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcifar10_train_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mpr\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/nn_assignments/libs/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 3\n",
    "c2_o = 100\n",
    "c2_f = 5\n",
    "hidden1 = 1500\n",
    "hidden2 = 1200\n",
    "hidden3 = 2500\n",
    "hidden4 = 2000\n",
    "hidden5 = 1000\n",
    "outs = 10\n",
    "gamma = 0.003\n",
    "alpha = 0.005\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 50\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c2_f, initC, \"Conv1\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, initW, gamma, \"tA1\"), \n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden2, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden3, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden3, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, initW, gamma, \"tA\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden5, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, = net.trainFunction(X, Y.ravel()) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'float'>\n",
      "Conv filter (50, 3, 5, 5)\n",
      "maxPool (50, 9, 9)\n",
      "Conv filter (100, 50, 5, 5)\n",
      "AffineLayer:  (2500, 1500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (1500, 1200)\n",
      "AffineLayer:  (1200, 2500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (2500, 2000)\n",
      "AffineLayer:  (2000, 1000)\n",
      "AffineLayer:  (1000, 10)\n",
      "Start\n",
      "gamma:  0.003\n",
      "alpha:  0.005\n",
      "28.8212689743 0.75\n",
      "28.5534723682 0.71\n",
      "28.3255749226 0.76\n",
      "28.0081979818 0.68\n",
      "After epoch:  0 0.6558 time:  29.4251060486\n",
      "27.7539963131 0.66\n",
      "27.5408900337 0.59\n",
      "27.2896728888 0.56\n",
      "27.1731458454 0.61\n",
      "After epoch:  1 0.5595 time:  29.3862380981\n",
      "27.0200676079 0.6\n",
      "26.7889199762 0.56\n",
      "26.5146210728 0.52\n",
      "26.3646943731 0.46\n",
      "After epoch:  2 0.5029 time:  29.3927900791\n",
      "26.1267358379 0.49\n",
      "26.1867808313 0.56\n",
      "25.762874012 0.47\n",
      "25.5814525661 0.4\n",
      "After epoch:  3 0.4778 time:  29.3873929977\n",
      "25.5643673515 0.49\n",
      "25.5977406425 0.48\n",
      "25.1781211843 0.35\n",
      "25.0971089964 0.37\n",
      "After epoch:  4 0.4558 time:  29.3930521011\n",
      "24.8887736349 0.41\n",
      "24.9025677319 0.39\n",
      "24.8407315931 0.45\n",
      "24.6134173546 0.42\n",
      "After epoch:  5 0.4325 time:  29.3873040676\n",
      "24.165945878 0.24\n",
      "24.196400279 0.31\n",
      "24.1434967852 0.34\n",
      "24.0138277187 0.4\n",
      "After epoch:  6 0.422 time:  29.3904178143\n",
      "23.9233731594 0.35\n",
      "23.8773923359 0.43\n",
      "23.6764535446 0.37\n",
      "23.562466157 0.35\n",
      "After epoch:  7 0.4083 time:  29.3858499527\n",
      "23.3857379327 0.41\n",
      "23.2213605566 0.31\n",
      "23.189246408 0.37\n",
      "22.9199010768 0.25\n",
      "After epoch:  8 0.3979 time:  29.3908779621\n",
      "23.1196744633 0.39\n",
      "22.9659965258 0.36\n",
      "22.977613739 0.36\n",
      "22.6682466803 0.35\n",
      "After epoch:  9 0.3881 time:  29.3869459629\n",
      "22.3861693702 0.23\n",
      "22.5494835768 0.36\n",
      "22.0884931321 0.26\n",
      "22.2572090378 0.33\n",
      "After epoch:  10 0.381 time:  29.3917341232\n",
      "22.0481972728 0.26\n",
      "21.9407054496 0.29\n",
      "22.0067148819 0.33\n",
      "21.7652806277 0.27\n",
      "After epoch:  11 0.3761 time:  29.3875739574\n",
      "21.6754774261 0.29\n",
      "21.5972119713 0.3\n",
      "21.6060775652 0.34\n",
      "21.271452282 0.23\n",
      "After epoch:  12 0.3733 time:  29.3915469646\n",
      "21.2637991991 0.26\n",
      "21.082788785 0.18\n",
      "21.1177033839 0.29\n",
      "20.9498031921 0.23\n",
      "After epoch:  13 0.3666 time:  29.3877179623\n",
      "21.1103824129 0.29\n",
      "20.8667564778 0.26\n",
      "20.7026385498 0.22\n",
      "20.5866448312 0.17\n",
      "After epoch:  14 0.3662 time:  29.3932609558\n",
      "20.540608048 0.23\n",
      "20.4160665975 0.19\n",
      "20.3354509282 0.21\n",
      "20.2754020519 0.2\n",
      "After epoch:  15 0.3613 time:  29.3866009712\n",
      "19.9986749303 0.07\n",
      "20.1364524474 0.18\n",
      "20.1415974607 0.23\n",
      "19.9939679098 0.19\n",
      "After epoch:  16 0.3667 time:  29.393020153\n",
      "19.9589047098 0.23\n",
      "19.7529389672 0.19\n",
      "19.76459657 0.19\n",
      "19.6134508004 0.18\n",
      "After epoch:  17 0.3664 time:  29.3864531517\n",
      "19.5628911767 0.18\n",
      "19.3557954113 0.12\n",
      "19.5104578223 0.21\n",
      "19.3678827376 0.23\n",
      "After epoch:  18 0.3679 time:  29.390994072\n",
      "19.2281953018 0.14\n",
      "19.2054497466 0.18\n",
      "18.9565917156 0.06\n",
      "18.9279439907 0.09\n",
      "After epoch:  19 0.3647 time:  29.3879389763\n",
      "18.8070373147 0.07\n",
      "18.7672350171 0.08\n",
      "18.9711644034 0.15\n",
      "18.7072092731 0.13\n",
      "After epoch:  20 0.361 time:  29.3920469284\n",
      "18.7004593644 0.15\n",
      "18.668294004 0.15\n",
      "18.5389069655 0.12\n",
      "18.5313552854 0.18\n",
      "After epoch:  21 0.3654 time:  29.3881020546\n",
      "18.3282445836 0.09\n",
      "18.3355439315 0.12\n",
      "18.2141526043 0.1\n",
      "18.1479401827 0.07\n",
      "After epoch:  22 0.3656 time:  29.3872489929\n",
      "18.1577260675 0.1\n",
      "18.0082533721 0.07\n",
      "17.9557573559 0.07\n",
      "18.0095996106 0.1\n",
      "After epoch:  23 0.3737 time:  29.3627440929\n",
      "17.8345447264 0.05\n",
      "17.7888701503 0.06\n",
      "17.7296075077 0.05\n",
      "17.7285535541 0.08\n",
      "After epoch:  24 0.3711 time:  29.3620359898\n",
      "17.5318022155 0.04\n",
      "17.5487210133 0.04\n",
      "17.619218044 0.1\n",
      "17.464503858 0.05\n",
      "After epoch:  25 0.3694 time:  29.3592858315\n",
      "17.3724719572 0.04\n",
      "17.292546155 0.01\n",
      "17.2316360547 0.02\n",
      "17.2489216491 0.04\n",
      "After epoch:  26 0.3704 time:  29.3603270054\n",
      "17.1885745971 0.03\n",
      "17.1118821957 0.04\n",
      "17.174794101 0.05\n",
      "17.0346365724 0.03\n",
      "After epoch:  27 0.3677 time:  29.3586039543\n",
      "16.9165745212 0.01\n",
      "16.8989411789 0.02\n",
      "16.8969501182 0.05\n",
      "16.8564860189 0.02\n",
      "After epoch:  28 0.3675 time:  29.3614118099\n",
      "16.7695968927 0.03\n",
      "16.700610745 0.0\n",
      "16.7238361642 0.03\n",
      "16.6251510412 0.0\n",
      "After epoch:  29 0.3653 time:  29.359623909\n",
      "16.5697990083 0.02\n",
      "16.5139637441 0.01\n",
      "16.4813691844 0.01\n",
      "16.4333019085 0.0\n",
      "After epoch:  30 0.3642 time:  29.3610579967\n",
      "16.3898305769 0.02\n",
      "16.3353022724 0.0\n",
      "16.3183125425 0.02\n",
      "16.2638848222 0.0\n",
      "After epoch:  31 0.3644 time:  29.3579659462\n",
      "16.2265507303 0.0\n",
      "16.2263597478 0.03\n",
      "16.185345294 0.01\n",
      "16.1387881132 0.02\n",
      "After epoch:  32 0.3664 time:  29.360833168\n",
      "16.0594511246 0.0\n",
      "16.0313439793 0.0\n",
      "15.975348883 0.0\n",
      "15.9461711496 0.0\n",
      "After epoch:  33 0.3685 time:  29.3598370552\n",
      "15.8982030419 0.0\n",
      "15.8612899906 0.0\n",
      "15.8331232563 0.0\n",
      "15.7850748114 0.0\n",
      "After epoch:  34 0.3615 time:  29.3616809845\n",
      "15.7568335567 0.0\n",
      "15.7122359885 0.0\n",
      "15.6914006908 0.0\n",
      "15.6539565827 0.0\n",
      "After epoch:  35 0.3653 time:  29.358784914\n",
      "15.6107993381 0.0\n",
      "15.5791333332 0.0\n",
      "15.5434049258 0.0\n",
      "15.5170188046 0.0\n",
      "After epoch:  36 0.3678 time:  29.3611140251\n",
      "15.4851124177 0.0\n",
      "15.4630056393 0.0\n",
      "15.4146032523 0.01\n",
      "15.3963309333 0.0\n",
      "After epoch:  37 "
     ]
    }
   ],
   "source": [
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 3\n",
    "c2_o = 100\n",
    "c2_f = 5\n",
    "hidden1 = 1500\n",
    "hidden2 = 1200\n",
    "hidden3 = 2500\n",
    "hidden4 = 2000\n",
    "hidden5 = 1000\n",
    "outs = 10\n",
    "gamma = 0.003\n",
    "alpha = 0.005\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 50\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c2_f, initC, \"Conv1\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, initW, gamma, \"tA1\"), \n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden2, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden3, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden3, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, initW, gamma, \"tA\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden5, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, = net.trainFunction(X + u.generate(numpy.random, X.shape), Y.ravel()) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 3\n",
    "c2_o = 100\n",
    "c2_f = 5\n",
    "hidden1 = 1500\n",
    "hidden2 = 1200\n",
    "hidden3 = 2500\n",
    "hidden4 = 2000\n",
    "hidden5 = 1000\n",
    "outs = 10\n",
    "gamma = 0.003\n",
    "alpha = 0.005\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 50\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c2_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, initW, gamma, \"tA1\"), \n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden2, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden3, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden3, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, initW, gamma, \"tA\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden5, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, = net.trainFunction(X , Y.ravel()) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.27\n",
      "22.2184374504 0.25\n",
      "After epoch:  10 0.3783 time:  29.3749330044\n",
      "22.0886293435 0.25\n",
      "22.001179276 0.28\n",
      "22.0768219624 0.34\n",
      "21.906710783 0.36\n",
      "After epoch:  11 0.3696 time:  29.3447511196\n",
      "21.7319138603 0.3\n",
      "21.4842985559 0.21\n",
      "21.3746518869 0.22\n",
      "21.3993080239 0.26\n",
      "After epoch:  12 0.3646 time:  29.3457448483\n",
      "21.2365318155 0.23\n",
      "21.056635602 0.16\n",
      "21.046574193 0.19\n",
      "21.0572039719 0.29\n",
      "After epoch:  13 0.3621 time:  29.343585968\n",
      "20.8926315637 0.22\n",
      "20.7590068207 0.2\n",
      "20.864456377 0.24\n",
      "20.666518827 0.28\n",
      "After epoch:  14 0.3597 time:  29.3464579582\n",
      "20.464023128 0.2\n",
      "20.508782434 0.21\n",
      "20.4226004872 0.17\n",
      "20.3481724958 0.17\n",
      "After epoch:  15 0.3617 time:  29.3413469791\n",
      "20.0332682495 0.14\n",
      "20.2825598607 0.2\n",
      "19.9670508451 0.18\n",
      "19.9788673515 0.23\n",
      "After epoch:  16 0.3636 time:  29.3448309898\n",
      "19.7908584874 0.11\n",
      "19.8568601727 0.18\n",
      "19.7922747183 0.2\n",
      "19.7898434858 0.21\n",
      "After epoch:  17 0.3648 time:  29.3410298824\n",
      "19.5526448977 0.16\n",
      "19.5229635911 0.18\n",
      "19.4494173989 0.17\n",
      "19.2958652601 0.14\n",
      "After epoch:  18 0.3632 time:  29.3450319767\n",
      "19.138060333 0.11\n",
      "19.2497345095 0.17\n",
      "19.1165530381 0.09\n",
      "19.0882821126 0.14\n",
      "After epoch:  19 0.3607 time:  29.3429129124\n",
      "18.9260991657 0.12\n",
      "18.8373697407 0.1\n",
      "18.8813116686 0.16\n",
      "18.7195342252 0.14\n",
      "After epoch:  20 0.3688 time:  29.345482111\n",
      "18.6432137854 0.08\n",
      "18.5270402896 0.04\n",
      "18.5437423 0.1\n",
      "18.6096131694 0.14\n",
      "After epoch:  21 0.3634 time:  29.3416290283\n",
      "18.3493264844 0.09\n",
      "18.342880208 0.08\n",
      "18.2745868936 0.12\n",
      "18.3705629187 0.17\n",
      "After epoch:  22 0.3655 time:  29.3447179794\n",
      "18.0510619709 0.04\n",
      "18.0534132488 0.08\n",
      "18.0787809155 0.13\n",
      "18.0004180298 0.13\n",
      "After epoch:  23 0.3633 time:  29.3437941074\n",
      "17.902687103 0.07\n",
      "17.8074442799 0.05\n",
      "17.8057434566 0.09\n",
      "17.7072422175 0.06\n",
      "After epoch:  24 0.3625 time:  29.3460218906\n",
      "17.6368776684 0.06\n",
      "17.5796294454 0.08\n",
      "17.5708070935 0.06\n",
      "17.5140567409 0.06\n",
      "After epoch:  25 0.367 time:  29.342674017\n",
      "17.3814093601 0.04\n",
      "17.3212283442 0.03\n",
      "17.3705925727 0.11\n",
      "17.2351891912 0.05\n",
      "After epoch:  26"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a2c2e8dc426e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpr\u001b[0m  \u001b[1;33m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"After epoch: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_er\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifar10_validation_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"time: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0me\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-2489e31cb805>\u001b[0m in \u001b[0;36mcompute_er\u001b[1;34m(net, stream)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnum_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;31m#print predictions != Y.ravel()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mnum_errs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/nn_assignments/libs/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 3\n",
    "c2_o = 100\n",
    "c2_f = 5\n",
    "hidden1 = 1500\n",
    "hidden2 = 1200\n",
    "hidden3 = 2500\n",
    "hidden4 = 2000\n",
    "hidden5 = 1000\n",
    "outs = 10\n",
    "gamma = 0.003\n",
    "alpha = 0.005\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 50\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c2_f, initC, \"Conv1\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, initW, gamma, \"tA1\"), \n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden2, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden3, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden3, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, initW, gamma, \"tA\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden5, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.1)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, = net.trainFunction(X + u.generate(numpy.random, X.shape), Y.ravel()) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'float'>\n",
      "Conv filter (50, 3, 5, 5)\n",
      "maxPool (50, 9, 9)\n",
      "Conv filter (100, 50, 5, 5)\n",
      "AffineLayer:  (2500, 1500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (1500, 1200)\n",
      "AffineLayer:  (1200, 2500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (2500, 2000)\n",
      "AffineLayer:  (2000, 1000)\n",
      "AffineLayer:  (1000, 10)\n",
      "Start\n",
      "gamma:  0.003\n",
      "alpha:  0.003\n",
      "28.8906262512 0.69\n",
      "28.7555505428 0.76\n",
      "28.6465790691 0.74\n",
      "28.4763495998 0.76\n",
      "After epoch:  0 0.7196 time:  29.3690638542\n",
      "28.3564878483 0.79\n",
      "28.1685270958 0.71\n",
      "28.0013622055 0.63\n",
      "27.8244158039 0.59\n",
      "After epoch:  1 0.6488 time:  29.3416910172\n",
      "27.7385859079 0.63\n",
      "27.5573060837 0.58\n",
      "27.4350211325 0.55\n",
      "27.2524747276 0.53\n",
      "After epoch:  2 0.585 time:  29.3402769566\n",
      "27.2392432909 0.56\n",
      "27.0430137129 0.52\n",
      "26.9165180416 0.59\n",
      "26.9818340521 0.59\n",
      "After epoch:  3 0.5454 time:  29.3390109539\n",
      "26.8729083033 0.58\n",
      "26.5941492052 0.48\n",
      "26.5839362793 0.55\n",
      "26.6605544662 0.64\n",
      "After epoch:  4 0.5149 time:  29.3379859924\n",
      "26.4003236809 0.54\n",
      "26.2317354975 0.48\n",
      "26.1180188437 0.44\n",
      "26.0621802654 0.46\n",
      "After epoch:  5 0.4906 time:  29.3386061192\n",
      "25.8347037992 0.45\n",
      "25.8306918182 0.44\n",
      "25.7715025387 0.45\n",
      "25.6052847624 0.41\n",
      "After epoch:  6 0.4699 time:  29.3392779827\n",
      "25.6511906071 0.48\n",
      "25.415076582 0.37\n",
      "25.4283425808 0.44\n",
      "25.1954678907 0.37\n",
      "After epoch:  7 0.4557 time:  29.3380799294\n",
      "25.3873142576 0.48\n",
      "25.3100404758 0.48\n",
      "25.0454905558 0.44\n",
      "25.2506782732 0.46\n",
      "After epoch:  8 0.4432 time:  29.3385369778\n",
      "24.8688122473 0.42\n",
      "24.7934613552 0.37\n",
      "24.7054864531 0.38\n",
      "24.7041227436 0.4\n",
      "After epoch:  9 0.4301 time:  29.3386259079\n",
      "24.6340271177 0.34\n",
      "24.7198630676 0.44\n",
      "24.376984344 0.31\n",
      "24.514801013 0.41\n",
      "After epoch:  10 0.4209 time:  29.338545084\n",
      "24.3408385096 0.38\n",
      "24.3489115086 0.38\n",
      "24.2459573269 0.42\n",
      "24.1392471285 0.36\n",
      "After epoch:  11 0.4122 time:  29.3402168751\n",
      "23.8684528913 0.24\n",
      "23.987525465 0.39\n",
      "23.7578464279 0.35\n",
      "23.8035631227 0.28\n",
      "After epoch:  12 0.4036 time:  29.338629961\n",
      "23.7779878259 0.32\n",
      "23.6327347112 0.3\n",
      "23.6293123989 0.27\n",
      "23.653811451 0.28\n",
      "After epoch:  13 0.4009 time:  29.3386180401\n",
      "23.5241704144 0.33\n",
      "23.4956658359 0.36\n",
      "23.4849351544 0.37\n",
      "23.5007248659 0.35\n",
      "After epoch:  14 0.392 time:  29.3375680447\n",
      "23.303625021 0.33\n",
      "23.1256761332 0.29\n",
      "23.1421521397 0.29\n",
      "23.0335286264 0.29\n",
      "After epoch:  15 0.3877 time:  29.3371360302\n",
      "22.961203208 0.25\n",
      "22.9644340653 0.28\n",
      "23.030848341 0.3\n",
      "22.9269057155 0.34\n",
      "After epoch:  16 0.3859 time:  29.3382759094\n",
      "22.8493477988 0.31\n",
      "22.8027418456 0.3\n",
      "22.5818877373 0.24\n",
      "22.791310514 0.35\n",
      "After epoch:  17 0.3836 time:  29.3445360661\n",
      "22.5279514818 0.23\n",
      "22.4886206636 0.21\n",
      "22.4705456095 0.31\n",
      "22.4275281959 0.23\n",
      "After epoch:  18 0.3842 time:  29.3399350643\n",
      "22.388829114 0.23\n",
      "22.2713779368 0.24\n",
      "22.1918150268 0.21\n",
      "22.1744402542 0.25\n",
      "After epoch:  19 0.3853 time:  29.3377931118\n",
      "22.1678785868 0.27\n",
      "22.1911662021 0.32\n",
      "22.044913682 0.24\n",
      "21.9286405125 0.2\n",
      "After epoch:  20 0.375 time:  29.337113142\n",
      "21.8905089312 0.17\n",
      "21.9935806046 0.27\n",
      "21.8656451254 0.22\n",
      "21.6800983272 0.17\n",
      "After epoch:  21 0.3823 time:  29.3388009071\n",
      "21.7479184394 0.22\n",
      "21.6380461822 0.21\n",
      "21.8047832532 0.28\n",
      "21.5505477624 0.17\n",
      "After epoch:  22 0.3863 time:  29.3372731209\n",
      "21.5042580843 0.21\n",
      "21.539845077 0.22\n",
      "21.400363009 0.2\n",
      "21.37876302 0.19\n",
      "After epoch:  23 0.3853 time:  29.3389499187\n",
      "21.3113501801 0.14\n",
      "21.3420529585 0.21\n",
      "21.1208683879 0.14\n",
      "21.2741360745 0.2\n",
      "After epoch:  24 0.3762 time:  29.3373608589\n",
      "21.1544047179 0.15\n",
      "21.1532619638 0.17\n",
      "21.0817569499 0.15\n",
      "21.1058180761 0.16\n",
      "After epoch:  25 0.3811 time:  29.3380160332\n",
      "21.058750145 0.18\n",
      "20.9266321404 0.16\n",
      "21.1775682688 0.29\n",
      "20.8655765359 0.14\n",
      "After epoch:  26 0.3777 time:  29.3367509842\n",
      "20.9135362945 0.2\n",
      "20.7313169682 0.13\n",
      "20.8849004598 0.22\n",
      "20.7146494496 0.15\n",
      "After epoch:  27 0.3835 time:  29.3420951366\n",
      "20.6141127684 0.14\n",
      "20.7786771674 0.15\n",
      "20.5980104458 0.13\n",
      "20.6104240229 0.16\n",
      "After epoch:  28 0.3821 time:  29.340665102\n",
      "20.5664792535 0.12\n",
      "20.4302989986 0.14\n",
      "20.4777896144 0.14\n",
      "20.396711597 0.15\n",
      "After epoch:  29 0.3828 time:  29.3448331356\n",
      "20.2560194738 0.05\n",
      "20.449944829 0.16\n",
      "20.275488477 0.12\n",
      "20.2936161044 0.11\n",
      "After epoch:  30 0.3802 time:  29.337580204\n",
      "20.2059597306 0.13\n",
      "20.2098512621 0.11\n",
      "20.2457041652 0.12\n",
      "20.1379583704 0.1\n",
      "After epoch:  31 0.3843 time:  29.3374631405\n",
      "20.0621031325 0.07\n",
      "20.0460796227 0.1\n",
      "19.9909307811 0.05\n",
      "20.0306257915 0.12\n",
      "After epoch:  32 0.384 time:  29.3358540535\n",
      "19.9329929061 0.08\n",
      "19.914940021 0.08\n",
      "19.8624633009 0.09\n",
      "19.8459778199 0.07\n",
      "After epoch:  33 0.3819 time:  29.3387060165\n",
      "19.8068520107 0.04\n",
      "19.7600460801 0.05\n",
      "19.8096828074 0.11\n",
      "19.7370459661 0.05\n",
      "After epoch:  34 0.3834 time:  29.3370909691\n",
      "19.6682649136 0.07\n",
      "19.8009460447 0.08\n",
      "19.6655376544 0.09\n",
      "19.6164879341 0.08\n",
      "After epoch:  35 0.3852 time:  29.3373320103\n",
      "19.5255070333 0.08\n",
      "19.5814504912 0.06\n",
      "19.5835525653 0.07\n",
      "19.5703753052 0.09\n",
      "After epoch:  36 0.3853 time:  29.3369481564\n",
      "19.417333084 0.03\n",
      "19.3960329049 0.04\n",
      "19.4616923277 0.08\n",
      "19.3733853359 0.06\n",
      "After epoch:  37 0.3926 time:  29.337886095\n",
      "19.3177056396 0.04\n",
      "19.3403368225 0.06\n",
      "19.3001693857 0.06\n",
      "19.2677564517 0.02\n",
      "After epoch:  38 0.3861 time:  29.3370089531\n",
      "19.1720291288 0.03\n",
      "19.2483914037 0.04\n",
      "19.2092650739 0.05\n",
      "19.1425327444 0.03\n",
      "After epoch:  39 0.3893 time:  29.3380389214\n",
      "19.0681805938 0.02\n",
      "19.1106619225 0.04\n",
      "19.0800826442 0.03\n",
      "19.1821043577 0.06\n",
      "After epoch:  40 0.3839 time:  29.3361980915\n",
      "19.0438419341 0.03\n",
      "19.0351604637 0.05\n",
      "19.0108351111 0.05\n",
      "18.9396212969 0.03\n",
      "After epoch:  41 0.3856 time:  29.3510458469\n",
      "18.935401967 0.04\n",
      "18.9148988798 0.03\n",
      "18.878513687 0.02\n",
      "18.8294456483 0.0\n",
      "After epoch:  42 0.3831 time:  29.3367581367\n",
      "18.8090526747 0.01\n",
      "18.8016258383 0.02\n",
      "18.8354106069 0.04\n",
      "18.7983794302 0.02\n",
      "After epoch:  43 0.3849 time:  29.3370819092\n",
      "18.7984252868 0.02\n",
      "18.6920264804 0.0\n",
      "18.7385730047 0.04\n",
      "18.6879073306 0.02\n",
      "After epoch:  44 0.3914 time:  29.3374011517\n",
      "18.6674886796 0.01\n",
      "18.6748998017 0.03\n",
      "18.6003380452 0.01\n",
      "18.5848392521 0.01\n",
      "After epoch:  45 0.3893 time:  29.337928772\n",
      "18.6004006804 0.02\n",
      "18.5549889396 0.0\n",
      "18.5635948538 0.02\n",
      "18.5354982545 0.02\n",
      "After epoch:  46 0.3872 time:  29.3380880356\n",
      "18.5089450285 0.02\n",
      "18.4541793204 0.0\n",
      "18.4790455149 0.01\n",
      "18.4734399348 0.02\n",
      "After epoch:  47 0.3886 time:  29.3374910355\n",
      "18.4287388954 0.01\n",
      "18.3969040319 0.01\n",
      "18.3807993727 0.01\n",
      "18.4005533858 0.04\n",
      "After epoch:  48 0.3894 time:  29.3373138905\n",
      "18.3408242289 0.0\n",
      "18.3781883689 0.03\n",
      "18.3151878806 0.01\n",
      "18.3093542564 0.0\n",
      "After epoch:  49 0.3911 time:  29.3381741047\n"
     ]
    }
   ],
   "source": [
    "img_size = (32, 32)\n",
    "c1_i = 3\n",
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 3\n",
    "c2_o = 100\n",
    "c2_f = 5\n",
    "hidden1 = 1500\n",
    "hidden2 = 1200\n",
    "hidden3 = 2500\n",
    "hidden4 = 2000\n",
    "hidden5 = 1000\n",
    "outs = 10\n",
    "gamma = 0.003\n",
    "alpha = 0.003\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 50\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c2_f, initC, \"Conv1\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, initW, gamma, \"tA1\"), \n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden2, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden3, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden3, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, initW, gamma, \"tA\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden5, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, = net.trainFunction(X + u.generate(numpy.random, X.shape), Y.ravel()) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'float'>\n",
      "Conv filter (50, 3, 5, 5)\n",
      "maxPool (50, 9, 9)\n",
      "Conv filter (100, 50, 5, 5)\n",
      "maxPool (100, 1, 1)\n",
      "AffineLayer:  (100, 1200)\n",
      "AffineLayer:  (1200, 2500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (2500, 1000)\n",
      "AffineLayer:  (1000, 10)\n",
      "Start\n",
      "gamma:  0.003\n",
      "alpha:  0.005\n",
      "14.0355478382 0.75\n",
      "13.8781158695 0.68\n",
      "13.6647407007 0.61\n",
      "13.5452297859 0.61\n",
      "After epoch:  0 0.6498 time:  20.2297329903\n",
      "13.4000119324 0.64\n",
      "13.2335639524 0.62\n",
      "13.1682131453 0.62\n",
      "13.1399305668 0.62\n",
      "After epoch:  1 0.5877 time:  20.2084889412\n",
      "12.9640302515 0.57\n",
      "12.8924028683 0.61\n",
      "12.7934504585 0.63\n",
      "12.7042204962 0.58\n",
      "After epoch:  2 0.5553 time:  20.2117159367\n",
      "12.6900714188 0.58\n",
      "12.5555318394 0.59\n",
      "12.5725090628 0.49\n",
      "12.4071586103 0.56\n",
      "After epoch:  3 0.5348 time:  20.2089660168\n",
      "12.5106524553 0.55\n",
      "12.4343873367 0.58\n",
      "12.2590950508 0.5\n",
      "12.2280057507 0.51\n",
      "After epoch:  4 0.5245 time:  20.213411808\n",
      "12.0968634329 0.5\n",
      "11.9242234154 0.41\n",
      "11.9860299311 0.47\n",
      "11.8396515522 0.44\n",
      "After epoch:  5 0.5144 time:  20.2092700005\n",
      "11.7585558472 0.47\n",
      "11.8450428991 0.48\n",
      "11.8266851845 0.58\n",
      "11.5665681314 0.45\n",
      "After epoch:  6 0.4963 time:  20.2126381397\n",
      "11.5552663536 0.45\n",
      "11.4781452808 0.44\n",
      "11.5631407566 0.54\n",
      "11.3981276255 0.41\n",
      "After epoch:  7 0.4887 time:  20.2093129158\n",
      "11.3412186165 0.44\n",
      "11.3039709368 0.42\n",
      "11.175416954 0.46\n",
      "11.1215628366 0.45\n",
      "After epoch:  8 0.4836 time:  20.2125620842\n",
      "11.2284587145 0.5\n",
      "10.9732238874 0.41\n",
      "11.1090026073 0.46\n",
      "11.1077832623 0.5\n",
      "After epoch:  9 0.4732 time:  20.2067730427\n",
      "10.80683078 0.38\n",
      "10.9556424494 0.42\n",
      "10.9888438807 0.51\n",
      "10.747192502 0.38\n",
      "After epoch:  10 0.4705 time:  20.2349290848\n",
      "10.6147198696 0.38\n",
      "10.7649226799 0.44\n",
      "10.7479157085 0.53\n",
      "10.6793105049 0.47\n",
      "After epoch:  11 0.4635 time:  20.2778439522\n",
      "10.5695462933 0.38\n",
      "10.6594627666 0.44\n",
      "10.7353669853 0.51\n",
      "10.5486199198 0.44\n",
      "After epoch:  12 0.4612 time:  20.213424921\n",
      "10.4171444635 0.47\n",
      "10.2270026598 0.41\n",
      "10.3093349972 0.38\n",
      "10.3405799484 0.44\n",
      "After epoch:  13 0.4552 time:  20.2081298828\n",
      "10.1592814302 0.41\n",
      "10.0833012972 0.34\n",
      "10.091302804 0.38\n",
      "10.1468071766 0.43\n",
      "After epoch:  14 0.4447 time:  20.2116789818\n",
      "10.1272282228 0.43\n",
      "10.0094902449 0.33\n",
      "9.84692157793 0.3\n",
      "9.98856018734 0.4\n",
      "After epoch:  15 0.4465 time:  20.207930088\n",
      "9.91918985939 0.39\n",
      "9.84633142948 0.39\n",
      "9.85909546566 0.38\n",
      "9.79553931713 0.45\n",
      "After epoch:  16 0.4421 time:  20.213668108\n",
      "9.88539173889 0.43\n",
      "9.81325602531 0.46\n",
      "9.62086607361 0.36\n",
      "9.7609683733 0.4\n",
      "After epoch:  17 0.4391 time:  20.2082109451\n",
      "9.77888928413 0.49\n",
      "9.68317223549 0.42\n",
      "9.58239112663 0.37\n",
      "9.76961963654 0.39\n",
      "After epoch:  18 0.44 time:  20.2128479481\n",
      "9.44000537872 0.41\n",
      "9.50942258549 0.41\n",
      "9.41794587994 0.35\n",
      "9.50868058395 0.39\n",
      "After epoch:  19 0.4321 time:  20.2086539268\n",
      "9.46519985199 0.44\n",
      "9.3183626585 0.34\n",
      "9.40995911026 0.4\n",
      "9.25244544888 0.37\n",
      "After epoch:  20 0.4329 time:  20.2123551369\n",
      "9.18483078241 0.4\n",
      "9.19264867687 0.33\n",
      "9.07047165155 0.28\n",
      "9.21295932293 0.44\n",
      "After epoch:  21 0.4313 time:  20.2085659504\n",
      "9.11088093376 0.38\n",
      "8.93420494843 0.33\n",
      "8.9674944334 0.4\n",
      "9.01400610018 0.34\n",
      "After epoch:  22 0.4278 time:  20.2127861977\n",
      "9.02074912739 0.4\n",
      "8.96304210854 0.33\n",
      "8.8975706625 0.28\n",
      "8.92471514606 0.34\n",
      "After epoch:  23 0.429 time:  20.2081079483\n",
      "8.93330516815 0.44\n",
      "8.85521395445 0.38\n",
      "8.97818597412 0.4\n",
      "8.84969377708 0.34\n",
      "After epoch:  24 0.4268 time:  20.2123370171\n",
      "8.66925376654 0.3\n",
      "8.70677115583 0.31\n",
      "8.93770943356 0.46\n",
      "8.78666363811 0.36\n",
      "After epoch:  25 0.4254 time:  20.2076261044\n",
      "8.63807106256 0.3\n",
      "8.584968153 0.33\n",
      "8.61546670294 0.35\n",
      "8.5742564764 0.27\n",
      "After epoch:  26 0.4189 time:  20.2135632038\n",
      "8.60138688469 0.3\n",
      "8.58362006474 0.35\n",
      "8.53848339891 0.37\n",
      "8.35755305481 0.27\n",
      "After epoch:  27 0.4209 time:  20.2066159248\n",
      "8.57276023197 0.37\n",
      "8.55777529144 0.4\n",
      "8.36912575388 0.29\n",
      "8.43617149067 0.34\n",
      "After epoch:  28 0.4213 time:  20.2122480869\n",
      "8.25908604622 0.26\n",
      "8.24259664965 0.26\n",
      "8.42007931232 0.34\n",
      "8.3164344039 0.26\n",
      "After epoch:  29 0.4163 time:  20.2082939148\n",
      "8.40052463341 0.38\n",
      "8.22025689507 0.29\n",
      "8.40470727634 0.4\n",
      "8.47111923599 0.38\n",
      "After epoch:  30 0.419 time:  20.2111990452\n",
      "8.40866360474 0.36\n",
      "8.14585941315 0.27\n",
      "8.53896218872 0.4\n",
      "8.17625568724 0.3\n",
      "After epoch:  31 0.4163 time:  20.2074840069\n",
      "8.05081456995 0.28\n",
      "8.04714292049 0.32\n",
      "8.26892210197 0.41\n",
      "8.04496919489 0.27\n",
      "After epoch:  32 0.4188 time:  20.2120490074\n",
      "8.11890954113 0.27\n",
      "7.8771362195 0.25\n",
      "8.03331257772 0.26\n",
      "8.28538407707 0.44\n",
      "After epoch:  33 0.4133 time:  20.2063519955\n",
      "7.921672966 0.3\n",
      "8.09008197069 0.39\n",
      "7.8531076355 0.22\n",
      "8.00089812136 0.28\n",
      "After epoch:  34 0.4113 time:  20.2123501301\n",
      "7.97597344923 0.4\n",
      "8.08245772457 0.35\n",
      "7.80396480656 0.24\n",
      "7.79281811714 0.24\n",
      "After epoch:  35 0.4115 time:  20.2074608803\n",
      "7.73269508553 0.26\n",
      "8.07546717644 0.38\n",
      "7.90002468204 0.32\n",
      "7.85719371033 0.3\n",
      "After epoch:  36 0.417 time:  20.2111539841\n",
      "7.84809715223 0.34\n",
      "8.03976532078 0.4\n",
      "7.62077636814 0.23\n",
      "7.67107091188 0.24\n",
      "After epoch:  37 0.4137 time:  20.2089180946\n",
      "7.56355588675 0.23\n",
      "7.80361742163 0.43\n",
      "7.55269963884 0.2\n",
      "7.63433870792 0.27\n",
      "After epoch:  38 0.4152 time:  20.2136008739\n",
      "7.59646724558 0.3\n",
      "7.63708362484 0.29\n",
      "7.58536713076 0.26\n",
      "7.53658210421 0.26\n",
      "After epoch:  39 0.4183 time:  20.2068970203\n",
      "7.70460633183 0.31\n",
      "7.43553482294 0.26\n",
      "7.47112821388 0.23\n",
      "7.36540830469 0.19\n",
      "After epoch:  40 0.4125 time:  20.212720871\n",
      "7.49909654808 0.31\n",
      "7.39835062695 0.27\n",
      "7.56963981247 0.34\n",
      "7.53266334724 0.26\n",
      "After epoch:  41 0.4132 time:  20.2083981037\n",
      "7.37651119089 0.19\n",
      "7.42183923292 0.29\n",
      "7.47825040054 0.33\n",
      "7.37020591497 0.31\n",
      "After epoch:  42 0.4095 time:  20.2129299641\n",
      "7.25886827803 0.24\n",
      "7.32367306423 0.23\n",
      "7.414305933 0.28\n",
      "7.32548075056 0.25\n",
      "After epoch:  43 0.4142 time:  20.2082359791\n",
      "7.35110413408 0.23\n",
      "7.222810812 0.23\n",
      "7.22593353319 0.24\n",
      "7.35081562424 0.34\n",
      "After epoch:  44 0.4157 time:  20.2116279602\n",
      "7.19768504429 0.25\n",
      "7.31836589766 0.29\n",
      "7.32325305891 0.29\n",
      "7.206861269 0.3\n",
      "After epoch:  45 0.4129 time:  20.2075068951\n",
      "7.31561582422 0.32\n",
      "7.27762090778 0.27\n",
      "7.23582490349 0.26\n",
      "7.0585426755 0.17\n",
      "After epoch:  46 0.4133 time:  20.2131140232\n",
      "7.1728798542 0.24\n",
      "7.10374525738 0.2\n",
      "7.09498692894 0.2\n",
      "7.1994847827 0.24\n",
      "After epoch:  47 0.4135 time:  20.2068901062\n",
      "7.13895801163 0.21\n",
      "7.07433961821 0.22\n",
      "6.99168667603 0.17\n",
      "7.1427136631 0.26\n",
      "After epoch:  48 0.4163 time:  20.2118628025\n",
      "7.07957254839 0.25\n",
      "7.23819463587 0.32\n",
      "6.9989695549 0.24\n",
      "7.04204448175 0.25\n",
      "After epoch:  49 0.4128 time:  20.2064628601\n"
     ]
    }
   ],
   "source": [
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 3\n",
    "c2_o = 100\n",
    "c2_f = 5\n",
    "hidden1 = 1500\n",
    "hidden2 = 1200\n",
    "hidden3 = 2500\n",
    "hidden4 = 1000\n",
    "hidden5 = 1000\n",
    "outs = 10\n",
    "gamma = 0.003\n",
    "alpha = 0.005\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 50\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c2_f, initC, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden2, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden3, initW, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden3, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      AffineLayer(hidden4, initW, gamma, \"tA\"),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, = net.trainFunction(X , Y.ravel()) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def xent(x , y):\n",
    "        return  -y * T.log(x) - (1-y) * T.log(1-x)\n",
    "    \n",
    "    def costF(x, w):\n",
    "        return x.mean() + 0.01 * (w ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv' object has no attribute 'F'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-7169afa264fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-f4364401003f>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv' object has no attribute 'F'"
     ]
    }
   ],
   "source": [
    "conv = Conv(3, 32, 3)\n",
    "flatten = Flatten()\n",
    "x = T.tensor4(\"x\")\n",
    "y = T.vector(\"y\", dtype='int64')\n",
    "\n",
    "C = conv.build(x)\n",
    "F = flatten.build(C)\n",
    "\n",
    "foo = theano.function(inputs=[x], \n",
    "                    outputs=[C, F])\n",
    "\n",
    "for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "    print X.shape\n",
    "    c,f = foo(X)\n",
    "    print c.shape\n",
    "    print f.shape\n",
    "    break\n",
    "\n",
    "conv = Conv(1, 1)\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\", dtype='int64')\n",
    "\n",
    "x = conv.build(x)\n",
    "for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "    X = X.T\n",
    "    Y = X.copy()\n",
    "    print X.shape\n",
    "    zeros(X.shape)\n",
    "    conv.resize(X)\n",
    "    print X.shape\n",
    "    print Y.shape\n",
    "    conv.reresize(X)\n",
    "    print X.shape\n",
    "    print X == Y\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fA  Shape.0\n",
      "sA  Shape.0\n",
      "tA  Shape.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x must be 1-d or 2-d tensor of floats. Got TensorType(float32, 4D)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-351-69855bf18bf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                       \u001b[0mAffineLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                       SoftMaxLayer(\"fSoftMax\")], alpha)\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Start\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-348-89d17fe8b490>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-348-89d17fe8b490>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFeedForwardNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/temp/nn_assignments/libs/Theano/theano/tensor/nnet/nnet.pyc\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(c)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msoftmax_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/temp/nn_assignments/libs/Theano/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \"\"\"\n\u001b[0;32m    599\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'return_list'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_test_value\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'off'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/temp/nn_assignments/libs/Theano/theano/tensor/nnet/nnet.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_dtypes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             raise ValueError('x must be 1-d or 2-d tensor of floats. Got %s' %\n\u001b[1;32m--> 431\u001b[1;33m                              x.type)\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_padleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_ones\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x must be 1-d or 2-d tensor of floats. Got TensorType(float32, 4D)"
     ]
    }
   ],
   "source": [
    "feats = 784\n",
    "hidden1 = 500\n",
    "hidden2 = 200\n",
    "outs = 10\n",
    "gamma = 0.001\n",
    "alpha = 0.1\n",
    "num_epochs  = 100\n",
    "\n",
    "net = FeedForwardNet([AffineLayer(feats, hidden1, gamma, \"fA\"), \n",
    "                      TanhLayer(\"fTanh\"),\n",
    "                      AffineLayer(hidden1, hidden2, gamma, \"sA\"), \n",
    "                      TanhLayer(\"fTanh\"),\n",
    "                      AffineLayer(hidden2, outs, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha)\n",
    "net.build()\n",
    "print \"Start\"\n",
    "i = 0\n",
    "e = 0\n",
    "while e < num_epochs:\n",
    "    for X, Y in mnist_train_stream.get_epoch_iterator():\n",
    "        pr ,c = net.trainFunction(X.T, Y.ravel())\n",
    "        i+=1\n",
    "        #if i % 100 == 0:\n",
    "            #print c, (pr  == Y).mean()\n",
    "    \n",
    "    print \"After epoch: \", e, compute_er(net, mnist_validation_stream)\n",
    "    e+=1\n",
    "\n",
    "    \n",
    "for X, Y in mnist_validation_stream.get_epoch_iterator():\n",
    "    predictions = net.predictFunction(X.T)\n",
    "    num_errs += (predictions != Y).sum()\n",
    "    num_examples += X.shape[1]\n",
    "    k+=1\n",
    "print num_errs, num_examples, k, num_errs/num_examples\n",
    "#print (iris_test_t  == net.predictFunction(iris_test_f)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.printing as TP\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))\n",
    "\n",
    "\n",
    "\n",
    "feats = 4\n",
    "hidden = 500\n",
    "outs = 3\n",
    "gamma = 0.001\n",
    "alpha = 0.1\n",
    "\n",
    "net = FeedForwardNet([AffineLayer(feats, hidden, gamma, \"fA\"), \n",
    "          TanhLayer(\"fTanh\"),\n",
    "          AffineLayer(hidden, outs, gamma, \"sA\"), \n",
    "          SoftMaxLayer(\"fSoftMax\")], alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#theano.printing.pydotprint(costFoo, outfile=\"symbolic_graph_unopt.png\", var_with_name_simple=True)  \n",
    "\n",
    "\n",
    "net.build()\n",
    "\n",
    "print iris_train_t\n",
    "for i in range(10000):\n",
    "    pr ,c = net.trainFunction(iris_train_f, iris_train_t)\n",
    "    if i % 100 == 0:\n",
    "        print c, (pr  == iris_train_t).mean()\n",
    "\n",
    "print (pr  == iris_train_t).mean()\n",
    "print pr\n",
    "\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=iris_test_t.ravel(), cmap='prism')\n",
    "subplot(2,1,2)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=net.predicFunction(iris_test_f), cmap='prism')\n",
    "print (iris_test_t  == net.predictFunction(iris_test_f)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print iris_test_t\n",
    "\n",
    "w = theano.shared(rng.randn(feats)*0.01, name=\"w\")\n",
    "#print w.get_value()\n",
    "b = theano.shared(0., name=\"b\")\n",
    "#print b.get_value()\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "\n",
    "p_1 = T.nnet.sigmoid(T.dot(x, w) + b)   # Probability that target = 1\n",
    "prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "c = costF(xent(p_1, y), w)# The cost to minimize\n",
    "gw, gb = T.grad(c, [w, b])             # Compute the gradient of the cost\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, c],\n",
    "          updates=((w, w - alpha * gw), (b, b - alpha * gb)))\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    p, c =train(iris_train_f, iris_train_t)\n",
    "    #print p, c, x.mean()\n",
    "    \n",
    "print (predict(iris_test_f) == iris_test_t).mean()\n",
    "\n",
    "\n",
    "#foo = theano.function(inputs=[iris_train_f], outputs=[f])\n",
    "\n",
    "#print iris_test_t\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=iris_test_t.ravel(), cmap='spring')\n",
    "subplot(2,1,2)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=predict(iris_test_f).ravel(), cmap='spring')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.printing as TP\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))\n",
    "\n",
    "\n",
    "\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "feats = 4\n",
    "hidden = 500\n",
    "outs = 1\n",
    "gamma = 0.1\n",
    "alpha = 0.1\n",
    "\n",
    "fAL = AffineLayer(feats, hidden, gamma, \"fA\")\n",
    "tL = TanhLayer()\n",
    "sAL = AffineLayer(hidden, outs, gamma, \"sA\")\n",
    "lL = LogRegLayer()\n",
    "\n",
    "fa = fAL.build(x)\n",
    "t = tL.build(fa)\n",
    "sa = sAL.build(t)\n",
    "out = lL.build(sa)\n",
    "pred = out > 0.5\n",
    "c = xent(out.ravel(), y).mean() + fAL.cost() + sAL.cost()\n",
    "\n",
    "theano.printing.pydotprint(out, outfile=\"symbolic_graph_unopt.png\", var_with_name_simple=True)  \n",
    "fgw, fgb = T.grad(c, fAL.parameters)\n",
    "sgw, sgb = T.grad(c, sAL.parameters)\n",
    "\n",
    "train = theano.function(inputs=[x,y], \n",
    "                        outputs=[pred, c], \n",
    "                        updates=(fAL.update(c, alpha) + sAL.update(c, alpha)))\n",
    "predict  = theano.function(inputs=[x], \n",
    "                        outputs=[pred])\n",
    "\n",
    "for i in range(100):\n",
    "    pr, cost = train(iris_train_f, iris_train_t)\n",
    "\n",
    "print (pr.ravel() == iris_train_t).mean()\n",
    "\n",
    "\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=iris_test_t.ravel(), cmap='spring')\n",
    "subplot(2,1,2)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=predict(iris_test_f), cmap='spring')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = np.array([[12, 13], [1, 3]])\n",
    "y1 = [1, 2]\n",
    "print x.shape\n",
    "y = T.vector()\n",
    "x = T.matrix()\n",
    "f = theano.function(inputs=[x, y], outputs=x+y)\n",
    "f(x1, y1)\n",
    "\n",
    "\n",
    "x2 = [1,2]\n",
    "y2 = [2,3]\n",
    "x2 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = (32, 32)\n",
    "c1_i = 3\n",
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 2\n",
    "c2_o = 10\n",
    "c2_f = 3\n",
    "hidden1 = 300\n",
    "hidden2 = 500\n",
    "hidden3 = 50\n",
    "hidden4 = 50\n",
    "hidden5 = 300\n",
    "outs = 10\n",
    "gamma = 0.001\n",
    "alpha = 0.01\n",
    "lamb = 0.1\n",
    "num_epochs  = 300\n",
    "\n",
    "net = FeedForwardNet([\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden3, gamma, \"tA1\"), \n",
    "                      BNLayer(hidden3, 'BN1'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, gamma, \"tA\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, gg1, gb1, gamma1, beta1, mean1, std1, gg2, gb2, gamma2, beta2, mean2, std2 = net.trainFunction(X , Y.ravel()) # + u.generate(numpy.random, X.shape)\n",
    "        i+=1\n",
    "        if i % 1 == 0:\n",
    "            print 'L1gg', gg1.shape, np.array(gg1)\n",
    "            print 'L1gb', gb1.shape, np.array(gb1)\n",
    "            print 'L1gamma', gamma1.shape, np.array(gamma1)\n",
    "            print 'L1beta', beta1.shape, np.array(beta1)\n",
    "            print 'L1mean', mean1.shape, np.array(mean1)\n",
    "            print 'L1std', std1.shape, np.array(std1)\n",
    "            print 'L2gg', gg2.shape, np.array(gg2)\n",
    "            print 'L2gb', gb2.shape, np.array(gb2)\n",
    "            print 'L2gamma', gamma2.shape, np.array(gamma2)\n",
    "            print 'L2beta', beta2.shape, np.array(beta2)\n",
    "            print 'L2mean', mean2.shape, np.array(mean2)\n",
    "            print 'L2std', std2.shape, np.array(std2)\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "        if i % 3 == 0:\n",
    "            break\n",
    "    break\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor.signal.downsample as down\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, lamb = 0.1,rng=None, name=\"\"):\n",
    "        self.name = name\n",
    "        self.lamb = lamb\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "    def update(self, foo, alpha):\n",
    "        return []\n",
    "    def cost(self):\n",
    "        return 0;\n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = inputDim\n",
    "    def getOutputDim(self):\n",
    "        return self.num_out\n",
    "    def setMoments(self, moments):\n",
    "        self.moments = moments\n",
    "    def setLambda(self, lamb):\n",
    "        self.lamb = lamb\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_out, gamma  = 0.1, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(20. / (2* num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.b = theano.shared(bias_init.generate(self.rng, (num_out)), name=self.name +\" bias\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    @property\n",
    "    def parametersValues(self):\n",
    "        return [self.W.get_value(), self.b.get_value()]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def build(self, X):\n",
    "        #print self.name+ \" \",X.shape \n",
    "        return X.dot(self.W) + self.b\n",
    "    def cost(self):\n",
    "        return  (self.W ** 2).sum() * self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw, gb = T.grad(foo, self.parameters)\n",
    "        moments = self.moments\n",
    "        self.setMoments((gw, gb))\n",
    "        return  [(self.W, self.W - (alpha * gw + self.lamb * moments[0])), \n",
    "                 (self.b, self.b - (alpha * gb+ self.lamb * moments[1]))]\n",
    "    def setInputDim(self, inputDim):\n",
    "        shape = (inputDim, self.num_out)\n",
    "        print \"AffineLayer: \", shape\n",
    "        self.W = theano.shared(self.weight_init.generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.setMoments(zeros(shape, dtype='float32'))\n",
    "\n",
    "class Affine2DLayer(Layer):\n",
    "    def __init__(self, num_out, gamma = None, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(Affine2DLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(6. / 2* (num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        if gamma is None:\n",
    "            self.gamma = theano.shared(0.1)\n",
    "        else:\n",
    "            self.gamma = theano.shared(gamma, name = self.name + \" gamma\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W]\n",
    "    @property\n",
    "    def parametersValues(self):\n",
    "        return [self.W.get_value()]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W']\n",
    "    \n",
    "    def build(self, X):\n",
    "        return X.dot(self.W)\n",
    "    def cost(self):\n",
    "        return  (self.W ** 2).sum() * self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw = T.grad(foo, self.parameters)\n",
    "        return  [(self.W, self.W -alpha * gw)] \n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = (self.num_out, inputDim[1], inputDim[2])\n",
    "        shape = inputDim +  self.num_out\n",
    "        print \"Affine2D\", shape\n",
    "        self.W = theano.shared(self.weight_init.generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.setMoments(zeros(self.W.shape, dtype='float32'))\n",
    "    \n",
    "class LogRegLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(LogRegLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.nnet.sigmoid(X)\n",
    "\n",
    "\n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(TanhLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        print \"tanh layer\", X\n",
    "        return T.tanh(X)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(ReLULayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, X)\n",
    "\n",
    "class Conv(Layer):\n",
    "    def __init__(self, f_out, f_size, gamma = 0.1, n = \"\", weight_init = None, **kwargs):\n",
    "        super(Conv, self).__init__(name = n, **kwargs)\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(50. / (2*f_out+ f_size + f_size))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.f_out = f_out\n",
    "        self.f_size = f_size\n",
    "    \n",
    "    \n",
    "    def setInputDim(self, inputDim):\n",
    "        F_size = (self.f_out, ) + (inputDim[0], self.f_size, self.f_size)                                   \n",
    "        self.num_out = (self.f_out, inputDim[1] - self.f_size + 1, inputDim[2] - self.f_size + 1)\n",
    "        print 'Conv filter', F_size\n",
    "        self.F = theano.shared(self.weight_init.generate(self.rng, F_size),name=self.name +\" filter\")\n",
    "        \n",
    "    def update(self, foo, alpha):\n",
    "        gf = T.grad(foo, self.F)\n",
    "        return  [(self.F, self.F - alpha * gf)]    \n",
    "    \n",
    "    def cost(self):\n",
    "        return  (self.F ** 2).sum() * self.gamma\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, T.nnet.conv2d(X, self.F))\n",
    "        \n",
    "        \n",
    "        \n",
    "class Flatten(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(Flatten, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.flatten(X, 2)\n",
    "    def setInputDim(self, inputDim):\n",
    "        out_dim = 1\n",
    "        for i in inputDim:\n",
    "            out_dim = out_dim * i\n",
    "        self.num_out = out_dim\n",
    "    \n",
    "\n",
    "class BNLayer(Layer):\n",
    "    def __init__(self,num_out, n = \"BNLayer\", gamma = 0.1, alpha=1.0,**kwargs):\n",
    "        super(BNLayer, self).__init__(name = n, **kwargs)\n",
    "        self.num_out, self.alpha = num_out, alpha\n",
    "        self.gamma= theano.shared(gamma)\n",
    "    def build(self, X):\n",
    "        self.Gamma = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Gamma \" + self.name))\n",
    "        print 'Gamma shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Beta  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Beta \" + self.name))\n",
    "        print 'Beta shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Gamma.tag.initializer = Constant(1.0)\n",
    "        self.Beta.tag.initializer = Constant(0.0)\n",
    "    \n",
    "        #self.stored_means = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Means\" + self.name))\n",
    "        #self.stored_stds  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Stds\" + self.name))\n",
    "        #self.stored_means.tag.initializer = Constant(0.0)\n",
    "        #self.stored_stds.tag.initializer = Constant(1.0)\n",
    "    \n",
    "        self.means = self.alpha * theano.tensor.mean(X, 0, keepdims=True)\n",
    "        self.stds = self.alpha * theano.tensor.std(X, 0, keepdims=True)\n",
    "        self.means.tag.initializer = Constant(0.0)\n",
    "        self.stds.tag.initializer = Constant(1.0)\n",
    "        #self.means = self.alpha *self.means + (1.0 - self.alpha) * self.stored_means.dimshuffle(0,'x')\n",
    "        #self.stds = self.alpha * self.stds + (1.0 - self.alpha) * self.stored_stds.dimshuffle(0,'x')\n",
    "        \n",
    "        normalized = theano.tensor.nnet.bn.batch_normalization(\n",
    "            X,\n",
    "            self.Gamma,\n",
    "            self.Beta,\n",
    "            self.means,\n",
    "            self.stds,\n",
    "            'high_mem'\n",
    "        )\n",
    "        return normalized\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.Gamma, self.Beta]\n",
    "    @property\n",
    "    def check(self):\n",
    "        return [self.gg, self.gb, self.Gamma, self.Beta,self.means, self.stds ]\n",
    "    #def cost(self):\n",
    "    #    return  ((self.Gamma ** 2).sum() + (self.Gamma ** 2).sum())* self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        self.gg, self.gb = T.grad(foo, self.parameters)\n",
    "        return  [(self.Gamma, self.Gamma- alpha *self.gg),\n",
    "            (self.Beta, self.Beta - alpha * self.gb)] \n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.nnet.softmax(X)\n",
    "\n",
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, p_size):\n",
    "        self.p_size = p_size\n",
    "    def build(self, input):\n",
    "        return down.max_pool_2d(input, (self.p_size,self.p_size), ignore_border=True)\n",
    "    def getOutputDim(self):\n",
    "        shape = (self.num_out[0], ) + (self.num_out[1]/self.p_size, self.num_out[2]/self.p_size) \n",
    "        print \"maxPool\", shape\n",
    "        return shape\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None, alpha=0.1, lamb = 0.1):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "        print type(alpha)\n",
    "        self.alpha = theano.shared(float32(alpha), name='alpha')\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def build(self, inputDim):\n",
    "        x = T.tensor4(\"x\")\n",
    "        y = T.vector(\"y\", dtype='int64')\n",
    "        cost = 0\n",
    "        paramUpdates = []\n",
    "        \n",
    "        X = x\n",
    "        o1 = []\n",
    "        o2 = []\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            #print inputDim\n",
    "            layer.setInputDim(inputDim)\n",
    "            layer.setLambda(lamb)\n",
    "            inputDim = layer.getOutputDim()\n",
    "            X = layer.build(X)\n",
    "            print 'name', layer.name\n",
    "            if i == 2:\n",
    "                print 'BNname', layer.name\n",
    "                o1 = layer\n",
    "            if i == 5:\n",
    "                print 'BNname', layer.name\n",
    "                o2 = layer\n",
    "\n",
    "            cost += layer.cost()\n",
    "        \n",
    "        pred = np.argmax(X, 1)\n",
    "        self.costFoo = T.nnet.categorical_crossentropy(X, y).mean() + cost\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            print layer.name\n",
    "            paramUpdates += layer.update(self.costFoo, self.alpha)\n",
    "            \n",
    "        o1 = o1.check\n",
    "        o2 = o2.check\n",
    "        \n",
    "        paramUpdates += [(self.alpha, self.alpha * 0.99993)]\n",
    "        self.train = theano.function(inputs=[x,y], \n",
    "                                    outputs=[pred, self.costFoo, self.alpha]+o1+o2,\n",
    "                                    updates=paramUpdates)\n",
    "        self.predict  = theano.function(inputs=[x], \n",
    "                                    outputs=pred)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def trainFunction(self):\n",
    "        return self.train\n",
    "    \n",
    "    @property\n",
    "    def predictFunction(self):\n",
    "        return self.predict\n",
    "    @property\n",
    "    def costFunction(self):\n",
    "        return self.costFoo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
